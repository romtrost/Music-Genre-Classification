{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:21:25.176222Z",
     "iopub.status.busy": "2022-05-23T17:21:25.175929Z",
     "iopub.status.idle": "2022-05-23T17:21:25.182977Z",
     "shell.execute_reply": "2022-05-23T17:21:25.182285Z",
     "shell.execute_reply.started": "2022-05-23T17:21:25.176170Z"
    }
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'data/genres_original/hiphop/hiphop.00005.wav'\n",
    "x , sr = librosa.load(audio_path)#x is an audio time series as a numpy array. sr is the sampling rate\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, genre_list, dataset):\n",
    "    \n",
    "    num_channels = 3\n",
    "    window_sizes = [25, 50, 100]\n",
    "    hop_sizes = [10, 25, 50]\n",
    "    \n",
    "    for genre in genre_list:\n",
    "        files = librosa.util.find_files(path+genre, ext=['wav'])#this returns the entire path for each file in a genre folder\n",
    "        \n",
    "        for song in files:\n",
    "            x , sr = librosa.load(song)\n",
    "            song_id = song[71+len(genre)+1:-4]#-4 - len(genre)\n",
    "            specs_ = []\n",
    "            \n",
    "            for i in range(num_channels):\n",
    "                \n",
    "                window_length = int(round(window_sizes[i]*sr/1000))\n",
    "                hop_length = int(round(hop_sizes[i]*sr/1000))\n",
    "\n",
    "                clip = torch.Tensor(x)\n",
    "                spec = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=2205, win_length=window_length, hop_length=hop_length, n_mels=128)(clip) #Check this otherwise use 2400\n",
    "                eps = 1e-6\n",
    "                spec = spec.numpy()\n",
    "                spec = np.log(spec+ eps)\n",
    "                spec = np.asarray(torchvision.transforms.Resize((128, 1500))(Image.fromarray(spec)))\n",
    "                specs_.append(spec)\n",
    "            \n",
    "            spec = np.dstack((specs_[0], specs_[1], specs_[2]))\n",
    "            dataset['song_id'].append(song_id)  \n",
    "            if song_id[-5:]!='25_30' or song_id[-5:]!='_0_50':\n",
    "                dataset[\"Mel_spectrograms\"].append(spec)  \n",
    "            dataset['label'].append(genre)\n",
    "                            \n",
    "                \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = { 'song_id':[], 'audio':[], 'Mel_spectrograms':[], 'label':[] }\n",
    "path = 'data/data_fiveSeconds/'\n",
    "genre_list = ['jazz', 'rock', 'hiphop', 'metal', 'pop', 'disco', 'blues', 'classical', 'country', 'reggae']\n",
    "data_diz = create_dataset(path, genre_list, dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('DATA_Mel_spectrograms.pkl', 'wb') as f:\n",
    "    pickle.dump(data_diz, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:21:25.184866Z",
     "iopub.status.busy": "2022-05-23T17:21:25.184412Z",
     "iopub.status.idle": "2022-05-23T17:21:42.602760Z",
     "shell.execute_reply": "2022-05-23T17:21:42.601888Z",
     "shell.execute_reply.started": "2022-05-23T17:21:25.184823Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../input/data-mels1/DATA_Mel_spectrograms_small.pkl', 'rb') as f:\n",
    "    DATA = pickle.load(f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:21:42.607064Z",
     "iopub.status.busy": "2022-05-23T17:21:42.606866Z",
     "iopub.status.idle": "2022-05-23T17:21:42.614753Z",
     "shell.execute_reply": "2022-05-23T17:21:42.614044Z",
     "shell.execute_reply.started": "2022-05-23T17:21:42.607038Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(inputs, targets, split_size):\n",
    "      \n",
    "    #scale the data\n",
    "    mean = inputs.mean(axis=(1, 2), keepdims=True)\n",
    "    std = inputs.std(axis=(1, 2), keepdims=True)\n",
    "    inputs = (inputs-mean)/std\n",
    "    \n",
    "    # Creating a validation set and a test set.\n",
    "    inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=split_size)\n",
    "    inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs_train, targets_train, \n",
    "                                                                              test_size=split_size)\n",
    "    \n",
    "    return inputs_train, inputs_val, inputs_test, targets_train, targets_val, targets_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(model, X, y, idx):\n",
    "    \n",
    "    genre_dict = {\n",
    "        0 : 'jazz',\n",
    "        1 : 'rock',\n",
    "        2 : 'hiphop',\n",
    "        3 : \"metal\",\n",
    "        4 : \"pop\",\n",
    "        5 : \"disco\",\n",
    "        6 : \"blues\",\n",
    "        7 : \"classical\",\n",
    "        8 : \"country\",\n",
    "        9 : \"reggae\",\n",
    "        }\n",
    "        \n",
    "    predictions = model.predict(X)\n",
    "    genre = np.argmax(predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:21:42.616474Z",
     "iopub.status.busy": "2022-05-23T17:21:42.616072Z",
     "iopub.status.idle": "2022-05-23T17:22:06.677181Z",
     "shell.execute_reply": "2022-05-23T17:22:06.676221Z",
     "shell.execute_reply.started": "2022-05-23T17:21:42.616434Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(DATA['Mel_spectrograms'])\n",
    "y = np.array(DATA['label'])\n",
    "y_encoded = pd.factorize(y.reshape(X.shape[0],))[0]\n",
    "y_encoded = y_encoded.reshape(X.shape[0],1)\n",
    "inputs_train, inputs_val, inputs_test, targets_train, targets_val, targets_test = prepare_datasets(X, y_encoded, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:22:06.679066Z",
     "iopub.status.busy": "2022-05-23T17:22:06.678768Z",
     "iopub.status.idle": "2022-05-23T17:23:39.877924Z",
     "shell.execute_reply": "2022-05-23T17:23:39.876991Z",
     "shell.execute_reply.started": "2022-05-23T17:22:06.679028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 17:22:06.816983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:06.819545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:06.820411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:06.824006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-23 17:22:06.824470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:06.825335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:06.826184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:11.867082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:11.868043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:11.868827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 17:22:11.871165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15045 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 1498, 32)     896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 749, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 63, 749, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 747, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 374, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 374, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 373, 32)       4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 187, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 187, 32)       128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 187, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 89760)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                5744704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,760,010\n",
      "Trainable params: 5,759,818\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 17:22:12.340270: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1863936000 exceeds 10% of free system memory.\n",
      "2022-05-23 17:22:14.503956: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1863936000 exceeds 10% of free system memory.\n",
      "2022-05-23 17:22:16.171813: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 17:22:18.011693: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - ETA: 0s - loss: 1.9928 - acc: 0.4203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 17:22:27.080400: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 230400000 exceeds 10% of free system memory.\n",
      "2022-05-23 17:22:27.431559: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 230400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 12s 140ms/step - loss: 1.9928 - acc: 0.4203 - val_loss: 2.0190 - val_acc: 0.2800\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 2s 96ms/step - loss: 0.4617 - acc: 0.8467 - val_loss: 3.1663 - val_acc: 0.1100\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.1963 - acc: 0.9518 - val_loss: 4.7350 - val_acc: 0.1900\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 3s 98ms/step - loss: 0.0681 - acc: 0.9901 - val_loss: 5.7696 - val_acc: 0.1300\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 3s 101ms/step - loss: 0.0303 - acc: 0.9975 - val_loss: 6.3986 - val_acc: 0.1700\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 2s 95ms/step - loss: 0.0288 - acc: 0.9951 - val_loss: 6.7812 - val_acc: 0.1100\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.0171 - acc: 0.9975 - val_loss: 8.4247 - val_acc: 0.1100\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.0242 - acc: 0.9938 - val_loss: 7.9852 - val_acc: 0.1100\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 3s 110ms/step - loss: 0.0252 - acc: 0.9963 - val_loss: 7.3646 - val_acc: 0.1400\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 3s 100ms/step - loss: 0.0271 - acc: 0.9926 - val_loss: 5.6968 - val_acc: 0.1600\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 2s 95ms/step - loss: 0.0175 - acc: 0.9988 - val_loss: 4.5075 - val_acc: 0.2100\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 2s 96ms/step - loss: 0.0101 - acc: 0.9988 - val_loss: 3.4611 - val_acc: 0.3300\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 3s 101ms/step - loss: 0.0170 - acc: 0.9975 - val_loss: 2.8392 - val_acc: 0.3700\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 2s 95ms/step - loss: 0.0157 - acc: 0.9988 - val_loss: 2.7129 - val_acc: 0.3700\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.0144 - acc: 0.9988 - val_loss: 1.6810 - val_acc: 0.5100\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 2s 95ms/step - loss: 0.0215 - acc: 0.9951 - val_loss: 1.3585 - val_acc: 0.5300\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 3s 97ms/step - loss: 0.0173 - acc: 0.9975 - val_loss: 1.3561 - val_acc: 0.5300\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 3s 99ms/step - loss: 0.0186 - acc: 0.9975 - val_loss: 1.2905 - val_acc: 0.5700\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 2s 93ms/step - loss: 0.0574 - acc: 0.9913 - val_loss: 1.2916 - val_acc: 0.6000\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.0343 - acc: 0.9926 - val_loss: 1.3189 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "def design_model(input_shape, targets):\n",
    "\n",
    "    # Let's design the model architecture.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'), \n",
    "        tf.keras.layers.Dense(len(np.unique(targets)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    model = design_model(inputs_train.shape[1:], y)\n",
    "\n",
    "    # Selection of the optimizer, loss type and metrics for performance evaluation.\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001),\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics = ['acc']\n",
    "                     )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Training the model.\n",
    "    history = model.fit(inputs_train, targets_train,\n",
    "                        validation_data=(inputs_val, targets_val),\n",
    "                        epochs=20,\n",
    "                        batch_size=32\n",
    "                        )\n",
    "\n",
    "    # Testing the model on never seen before data.\n",
    "    make_prediction(model, inputs_test, targets_test, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T17:26:58.369617Z",
     "iopub.status.busy": "2022-05-23T17:26:58.369269Z",
     "iopub.status.idle": "2022-05-23T17:29:37.987144Z",
     "shell.execute_reply": "2022-05-23T17:29:37.986129Z",
     "shell.execute_reply.started": "2022-05-23T17:26:58.369583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Functional)     (None, 4, 46, 1024)       7037504   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 23, 1024)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 23, 1024)       4096      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 23, 1024)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 47104)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                3014720   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 10,057,226\n",
      "Trainable params: 3,017,546\n",
      "Non-trainable params: 7,039,680\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "26/26 [==============================] - 15s 314ms/step - loss: 1.8612 - acc: 0.3820 - val_loss: 0.9650 - val_acc: 0.6500\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.8696 - acc: 0.7256 - val_loss: 0.7439 - val_acc: 0.7600\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.6299 - acc: 0.8269 - val_loss: 0.7135 - val_acc: 0.8500\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4562 - acc: 0.8937 - val_loss: 0.6927 - val_acc: 0.8400\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 0.3562 - acc: 0.9431 - val_loss: 0.6796 - val_acc: 0.8700\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.2823 - acc: 0.9543 - val_loss: 0.6792 - val_acc: 0.8600\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.2323 - acc: 0.9753 - val_loss: 0.6017 - val_acc: 0.8600\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 6s 218ms/step - loss: 0.2040 - acc: 0.9852 - val_loss: 0.5955 - val_acc: 0.8900\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 6s 218ms/step - loss: 0.1740 - acc: 0.9889 - val_loss: 0.5690 - val_acc: 0.9200\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.1610 - acc: 0.9926 - val_loss: 0.5470 - val_acc: 0.8900\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 6s 229ms/step - loss: 0.1455 - acc: 0.9901 - val_loss: 0.5306 - val_acc: 0.8900\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.1275 - acc: 0.9913 - val_loss: 0.5361 - val_acc: 0.8800\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 6s 223ms/step - loss: 0.1162 - acc: 0.9913 - val_loss: 0.5129 - val_acc: 0.8900\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 6s 221ms/step - loss: 0.1102 - acc: 0.9913 - val_loss: 0.4840 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 6s 221ms/step - loss: 0.1048 - acc: 0.9963 - val_loss: 0.4688 - val_acc: 0.9100\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 6s 223ms/step - loss: 0.1003 - acc: 0.9963 - val_loss: 0.4713 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 6s 223ms/step - loss: 0.0900 - acc: 0.9988 - val_loss: 0.4470 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 6s 223ms/step - loss: 0.0856 - acc: 0.9963 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.0807 - acc: 0.9963 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.0765 - acc: 0.9975 - val_loss: 0.4383 - val_acc: 0.8900\n"
     ]
    }
   ],
   "source": [
    "def design_model_1(input_shape, targets):\n",
    "    \n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(input_shape = input_shape, \n",
    "                                                            include_top = False, \n",
    "                                                            weights = \"imagenet\")\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Let's design the model architecture.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        base_model,\n",
    "        \n",
    "        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3), \n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'), \n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(len(np.unique(targets)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    model = design_model_1(inputs_train.shape[1:], y)\n",
    "\n",
    "    # Selection of the optimizer, loss type and metrics for performance evaluation.\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001),\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics = ['acc']\n",
    "                     )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Training the model.\n",
    "    history = model.fit(inputs_train, targets_train,\n",
    "                        validation_data=(inputs_val, targets_val),\n",
    "                        epochs=20,\n",
    "                        batch_size=32\n",
    "                        )\n",
    "\n",
    "    # Testing the model on never seen before data.\n",
    "    make_prediction(model, inputs_test, targets_test, 24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
